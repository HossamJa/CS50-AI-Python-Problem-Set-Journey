# Analysis

## Layer 2, Head 5

This head focuses on the Direct Object attending to the Verb it modifies. The attention mechanism seems to capture the relationship between a noun (the direct object, or Query token) and the main action verb (the Key token) that governs it. This is essential for understanding the core action and recipient in a transitive verb phrase.

Example Sentences:
- The programmer is writing **code** rapidly.
- She broke a fragile **vase** this morning. 

## Layer 3, Head 4

This head appears to focus on Prepositions attending to the Object of the Preposition. The attention head links a preposition (the Query token) to the head noun of the prepositional phrase (the Key token). This connection helps the model parse prepositional phrases and understand spatial or temporal relationships within a sentence.

Example Sentences:
- The bird is perched **on** the highest branch. 
- We walked **through** a thick forest near the river.

